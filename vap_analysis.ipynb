{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim: Trying to visualize the attention in a pretrained VAP model to see whether attention sinks occur here after some sample audios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from vap.modules.lightning_module import VAPModule\n",
    "from vap.data.datamodule import VAPDataModule\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serhan/miniconda3/envs/vap/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAPModule(\n",
       "  (model): VAP(\n",
       "    (encoder): EncoderCPC(\n",
       "      (encoder): CPCModel(\n",
       "        (gEncoder): CPCEncoder(\n",
       "          (conv0): Conv1d(1, 256, kernel_size=(10,), stride=(5,), padding=(3,))\n",
       "          (batchNorm0): ChannelNorm()\n",
       "          (conv1): Conv1d(256, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "          (batchNorm1): ChannelNorm()\n",
       "          (conv2): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (batchNorm2): ChannelNorm()\n",
       "          (conv3): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (batchNorm3): ChannelNorm()\n",
       "          (conv4): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (batchNorm4): ChannelNorm()\n",
       "        )\n",
       "        (gAR): CPCAR(\n",
       "          (baseNet): LSTM(256, 256, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Rearrange('b t d -> b d t')\n",
       "        (1): CConv1d(\n",
       "          256, 256, kernel_size=(5,), stride=(2,)\n",
       "          (pad): ConstantPad1d(padding=(4, 0), value=0)\n",
       "        )\n",
       "        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Rearrange('b d t -> b t d')\n",
       "      )\n",
       "    )\n",
       "    (transformer): TransformerStereo(\n",
       "      (ar_channel): GPT(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerLayer(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln_self_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_ffnetwork): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mha): MultiHeadAttentionAlibi(\n",
       "              (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (unstack_heads): Rearrange('b t (h d) -> b h t d', h=4)\n",
       "              (stack_heads): Rearrange('b h t d -> b t (h d)')\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (ffnetwork): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=768, bias=False)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=768, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ar): GPTStereo(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TransformerStereoLayer(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln_self_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_ffnetwork): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mha): MultiHeadAttentionAlibi(\n",
       "              (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (unstack_heads): Rearrange('b t (h d) -> b h t d', h=4)\n",
       "              (stack_heads): Rearrange('b h t d -> b t (h d)')\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (ffnetwork): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=768, bias=False)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=768, out_features=256, bias=False)\n",
       "            )\n",
       "            (ln_src_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mha_cross): MultiHeadAttentionAlibi(\n",
       "              (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (unstack_heads): Rearrange('b t (h d) -> b h t d', h=4)\n",
       "              (stack_heads): Rearrange('b h t d -> b t (h d)')\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (combinator): Combinator(\n",
       "          (h0_a): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (h0_b): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (objective): VAPObjective\n",
       "    Codebook(\n",
       "      (emb): Embedding(256, 8)\n",
       "    )\n",
       "    ProjectionWindow(\n",
       "      bin_times: [0.2, 0.4, 0.6, 0.8]\n",
       "      bin_frames: [10, 20, 30, 40]\n",
       "      frame_hz: 50\n",
       "      thresh: 0.5\n",
       "    )\n",
       "    \n",
       "    \n",
       "    (feature_projection): Identity()\n",
       "    (va_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (vap_head): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "checkpoint_path = \"/home/serhan/Desktop/VoiceActivityProjection/example/checkpoints/checkpoint.ckpt\"\n",
    "model = VAPModule.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a batch of audio samples\n",
    "audio_dir = \"/home/erik/projects/data/Fisher/fisher_eng_tr_sp_d1/audio/000\"\n",
    "audio_files = [f\"fe_03_{i:05d}.wav\" for i in range(1, 100)]\n",
    "batch_size = 4\n",
    "sample_rate = 8000  # Assuming 8kHz sample rate, adjust if different\n",
    "\n",
    "def load_and_process_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    # Assuming stereo audio, if mono, adjust accordingly\n",
    "    if waveform.shape[0] == 1:\n",
    "        waveform = waveform.repeat(2, 1)\n",
    "    # Truncate or pad to 10 seconds\n",
    "    target_length = 10 * sample_rate\n",
    "    if waveform.shape[1] < target_length:\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, target_length - waveform.shape[1]))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of audio samples\n",
    "batch = []\n",
    "for i in range(batch_size):\n",
    "    file_path = os.path.join(audio_dir, audio_files[i])\n",
    "    waveform = load_and_process_audio(file_path)\n",
    "    batch.append(waveform)\n",
    "\n",
    "batch = torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the batch through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract attention weights\n",
    "def get_attention_weights(module):\n",
    "    attention_weights = []\n",
    "    handles = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        attention_weights.append(output[1].detach())\n",
    "    \n",
    "    for name, layer in module.named_modules():\n",
    "        if isinstance(layer, torch.nn.MultiheadAttention):\n",
    "            handle = layer.register_forward_hook(hook)\n",
    "            handles.append(handle)\n",
    "    \n",
    "    return attention_weights, handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights\n",
    "attention_weights, handles = get_attention_weights(model.model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model again to get attention weights\n",
    "with torch.no_grad():\n",
    "    _ = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the hooks\n",
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of rows must be a positive integer, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Visualize attention weights\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(attention_weights)\n\u001b[0;32m----> 3\u001b[0m fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49msubplots(num_layers, \u001b[39m1\u001b[39;49m, figsize\u001b[39m=\u001b[39;49m(\u001b[39m10\u001b[39;49m, \u001b[39m5\u001b[39;49m \u001b[39m*\u001b[39;49m num_layers))\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m num_layers \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m      5\u001b[0m     axes \u001b[39m=\u001b[39m [axes]\n",
      "File \u001b[0;32m~/miniconda3/envs/vap/lib/python3.10/site-packages/matplotlib/pyplot.py:1614\u001b[0m, in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[39mCreate a figure and a set of subplots.\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \n\u001b[1;32m   1612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m fig \u001b[39m=\u001b[39m figure(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfig_kw)\n\u001b[0;32m-> 1614\u001b[0m axs \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49msubplots(nrows\u001b[39m=\u001b[39;49mnrows, ncols\u001b[39m=\u001b[39;49mncols, sharex\u001b[39m=\u001b[39;49msharex, sharey\u001b[39m=\u001b[39;49msharey,\n\u001b[1;32m   1615\u001b[0m                    squeeze\u001b[39m=\u001b[39;49msqueeze, subplot_kw\u001b[39m=\u001b[39;49msubplot_kw,\n\u001b[1;32m   1616\u001b[0m                    gridspec_kw\u001b[39m=\u001b[39;49mgridspec_kw, height_ratios\u001b[39m=\u001b[39;49mheight_ratios,\n\u001b[1;32m   1617\u001b[0m                    width_ratios\u001b[39m=\u001b[39;49mwidth_ratios)\n\u001b[1;32m   1618\u001b[0m \u001b[39mreturn\u001b[39;00m fig, axs\n",
      "File \u001b[0;32m~/miniconda3/envs/vap/lib/python3.10/site-packages/matplotlib/figure.py:930\u001b[0m, in \u001b[0;36mFigureBase.subplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwidth_ratios\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must not be defined both as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mparameter and as key in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mgridspec_kw\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    928\u001b[0m     gridspec_kw[\u001b[39m'\u001b[39m\u001b[39mwidth_ratios\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m width_ratios\n\u001b[0;32m--> 930\u001b[0m gs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_gridspec(nrows, ncols, figure\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgridspec_kw)\n\u001b[1;32m    931\u001b[0m axs \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39msubplots(sharex\u001b[39m=\u001b[39msharex, sharey\u001b[39m=\u001b[39msharey, squeeze\u001b[39m=\u001b[39msqueeze,\n\u001b[1;32m    932\u001b[0m                   subplot_kw\u001b[39m=\u001b[39msubplot_kw)\n\u001b[1;32m    933\u001b[0m \u001b[39mreturn\u001b[39;00m axs\n",
      "File \u001b[0;32m~/miniconda3/envs/vap/lib/python3.10/site-packages/matplotlib/figure.py:1542\u001b[0m, in \u001b[0;36mFigureBase.add_gridspec\u001b[0;34m(self, nrows, ncols, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[39mReturn a `.GridSpec` that has this figure as a parent.  This allows\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[39mcomplex layout of Axes in the figure.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \n\u001b[1;32m   1539\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# pop in case user has added this...\u001b[39;00m\n\u001b[0;32m-> 1542\u001b[0m gs \u001b[39m=\u001b[39m GridSpec(nrows\u001b[39m=\u001b[39;49mnrows, ncols\u001b[39m=\u001b[39;49mncols, figure\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mreturn\u001b[39;00m gs\n",
      "File \u001b[0;32m~/miniconda3/envs/vap/lib/python3.10/site-packages/matplotlib/gridspec.py:378\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhspace \u001b[39m=\u001b[39m hspace\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39m=\u001b[39m figure\n\u001b[0;32m--> 378\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(nrows, ncols,\n\u001b[1;32m    379\u001b[0m                  width_ratios\u001b[39m=\u001b[39;49mwidth_ratios,\n\u001b[1;32m    380\u001b[0m                  height_ratios\u001b[39m=\u001b[39;49mheight_ratios)\n",
      "File \u001b[0;32m~/miniconda3/envs/vap/lib/python3.10/site-packages/matplotlib/gridspec.py:48\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m    If not given, all rows will have the same height.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(nrows, Integral) \u001b[39mor\u001b[39;00m nrows \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of rows must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mnrows\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ncols, Integral) \u001b[39mor\u001b[39;00m ncols \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of columns must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mncols\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of rows must be a positive integer, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x0 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize attention weights\n",
    "num_layers = len(attention_weights)\n",
    "fig, axes = plt.subplots(num_layers, 1, figsize=(10, 5 * num_layers))\n",
    "if num_layers == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for layer, weights in enumerate(attention_weights):\n",
    "    avg_weights = weights.mean(0).cpu().numpy()\n",
    "    im = axes[layer].imshow(avg_weights, aspect='auto', cmap='viridis')\n",
    "    axes[layer].set_title(f'Average Attention Weights - Layer {layer+1}')\n",
    "    axes[layer].set_xlabel('Key Position')\n",
    "    axes[layer].set_ylabel('Query Position')\n",
    "    fig.colorbar(im, ax=axes[layer])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_weights.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze attention to initial tokens\n",
    "initial_token_attention = [weights[:, :, :4].mean(dim=(0, 1)).cpu().numpy() for weights in attention_weights]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for layer, attn in enumerate(initial_token_attention):\n",
    "    ax.plot(range(4), attn, label=f'Layer {layer+1}')\n",
    "ax.set_xlabel('Initial Token Position')\n",
    "ax.set_ylabel('Average Attention Weight')\n",
    "ax.set_title('Attention to Initial Tokens Across Layers')\n",
    "ax.legend()\n",
    "plt.savefig('initial_token_attention.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Analysis complete. Check 'attention_weights.png' and 'initial_token_attention.png' for visualizations.\")\n",
    "\n",
    "# Print statistics about attention to initial tokens\n",
    "print(\"\\nAverage attention to initial tokens:\")\n",
    "for layer, attn in enumerate(initial_token_attention):\n",
    "    print(f\"Layer {layer+1}: {attn.mean():.4f}\")\n",
    "\n",
    "print(\"\\nAttention sink analysis:\")\n",
    "for layer, attn in enumerate(initial_token_attention):\n",
    "    if attn[0] > attn[1:].mean() * 1.5:  # Arbitrary threshold, adjust as needed\n",
    "        print(f\"Layer {layer+1} shows strong attention sink behavior\")\n",
    "    else:\n",
    "        print(f\"Layer {layer+1} does not show strong attention sink behavior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from vap.modules.lightning_module import VAPModule\n",
    "from vap.modules.modules import MultiHeadAttentionAlibi\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAPModule(\n",
       "  (model): VAP(\n",
       "    (encoder): EncoderCPC(\n",
       "      (encoder): CPCModel(\n",
       "        (gEncoder): CPCEncoder(\n",
       "          (conv0): Conv1d(1, 256, kernel_size=(10,), stride=(5,), padding=(3,))\n",
       "          (batchNorm0): ChannelNorm()\n",
       "          (conv1): Conv1d(256, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "          (batchNorm1): ChannelNorm()\n",
       "          (conv2): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (batchNorm2): ChannelNorm()\n",
       "          (conv3): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (batchNorm3): ChannelNorm()\n",
       "          (conv4): Conv1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (batchNorm4): ChannelNorm()\n",
       "        )\n",
       "        (gAR): CPCAR(\n",
       "          (baseNet): LSTM(256, 256, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Rearrange('b t d -> b d t')\n",
       "        (1): CConv1d(\n",
       "          256, 256, kernel_size=(5,), stride=(2,)\n",
       "          (pad): ConstantPad1d(padding=(4, 0), value=0)\n",
       "        )\n",
       "        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Rearrange('b d t -> b t d')\n",
       "      )\n",
       "    )\n",
       "    (transformer): TransformerStereo(\n",
       "      (ar_channel): GPT(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerLayer(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln_self_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_ffnetwork): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mha): MultiHeadAttentionAlibi(\n",
       "              (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (unstack_heads): Rearrange('b t (h d) -> b h t d', h=4)\n",
       "              (stack_heads): Rearrange('b h t d -> b t (h d)')\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (ffnetwork): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=768, bias=False)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=768, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ar): GPTStereo(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TransformerStereoLayer(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln_self_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_ffnetwork): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mha): MultiHeadAttentionAlibi(\n",
       "              (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (unstack_heads): Rearrange('b t (h d) -> b h t d', h=4)\n",
       "              (stack_heads): Rearrange('b h t d -> b t (h d)')\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (ffnetwork): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=768, bias=False)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=768, out_features=256, bias=False)\n",
       "            )\n",
       "            (ln_src_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mha_cross): MultiHeadAttentionAlibi(\n",
       "              (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (unstack_heads): Rearrange('b t (h d) -> b h t d', h=4)\n",
       "              (stack_heads): Rearrange('b h t d -> b t (h d)')\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (combinator): Combinator(\n",
       "          (h0_a): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (h0_b): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (objective): VAPObjective\n",
       "    Codebook(\n",
       "      (emb): Embedding(256, 8)\n",
       "    )\n",
       "    ProjectionWindow(\n",
       "      bin_times: [0.2, 0.4, 0.6, 0.8]\n",
       "      bin_frames: [10, 20, 30, 40]\n",
       "      frame_hz: 50\n",
       "      thresh: 0.5\n",
       "    )\n",
       "    \n",
       "    \n",
       "    (feature_projection): Identity()\n",
       "    (va_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (vap_head): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "checkpoint_path = \"/home/serhan/Desktop/VoiceActivityProjection/example/checkpoints/checkpoint.ckpt\"\n",
    "model = VAPModule.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "encoder: <class 'vap.modules.encoder.EncoderCPC'>\n",
      "  encoder: <class 'vap.modules.encoder_components.CPCModel'>\n",
      "    gEncoder: <class 'vap.modules.encoder_components.CPCEncoder'>\n",
      "      conv0: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm0: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv1: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm1: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv2: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm2: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv3: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm3: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv4: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm4: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "    gAR: <class 'vap.modules.encoder_components.CPCAR'>\n",
      "      baseNet: <class 'torch.nn.modules.rnn.LSTM'>\n",
      "  downsample: <class 'torch.nn.modules.container.Sequential'>\n",
      "    0: <class 'einops.layers.torch.Rearrange'>\n",
      "    1: <class 'vap.modules.encoder_components.CConv1d'>\n",
      "      pad: <class 'torch.nn.modules.padding.ConstantPad1d'>\n",
      "    2: <class 'vap.modules.encoder_components.LayerNorm'>\n",
      "      ln: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "      in_rearrange: <class 'einops.layers.torch.Rearrange'>\n",
      "      out_rearrange: <class 'einops.layers.torch.Rearrange'>\n",
      "    3: <class 'torch.nn.modules.activation.GELU'>\n",
      "    4: <class 'einops.layers.torch.Rearrange'>\n",
      "transformer: <class 'vap.modules.modules.TransformerStereo'>\n",
      "  ar_channel: <class 'vap.modules.modules.GPT'>\n",
      "    layers: <class 'torch.nn.modules.container.ModuleList'>\n",
      "      0: <class 'vap.modules.modules.TransformerLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "  ar: <class 'vap.modules.modules.GPTStereo'>\n",
      "    layers: <class 'torch.nn.modules.container.ModuleList'>\n",
      "      0: <class 'vap.modules.modules.TransformerStereoLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ln_src_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha_cross: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "      1: <class 'vap.modules.modules.TransformerStereoLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ln_src_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha_cross: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "      2: <class 'vap.modules.modules.TransformerStereoLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ln_src_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha_cross: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "    combinator: <class 'vap.modules.modules.Combinator'>\n",
      "      h0_a: <class 'torch.nn.modules.linear.Linear'>\n",
      "      h0_b: <class 'torch.nn.modules.linear.Linear'>\n",
      "      ln: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "      activation: <class 'torch.nn.modules.activation.GELU'>\n",
      "objective: <class 'vap.objective.VAPObjective'>\n",
      "  codebook: <class 'vap.objective.Codebook'>\n",
      "    emb: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "feature_projection: <class 'torch.nn.modules.linear.Identity'>\n",
      "va_classifier: <class 'torch.nn.modules.linear.Linear'>\n",
      "vap_head: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# Inspect model structure\n",
    "def print_model_structure(model, indent=0):\n",
    "    for name, module in model.named_children():\n",
    "        print('  ' * indent + f\"{name}: {type(module)}\")\n",
    "        if list(module.children()):\n",
    "            print_model_structure(module, indent + 1)\n",
    "\n",
    "print(\"Model structure:\")\n",
    "print_model_structure(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(module):\n",
    "    attention_weights = []\n",
    "    handles = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        # Try to access attention weights directly from the module\n",
    "        if hasattr(module, 'last_attn_weights'):\n",
    "            attention_weights.append(module.last_attn_weights.detach())\n",
    "    \n",
    "    for name, layer in module.named_modules():\n",
    "        if isinstance(layer, MultiHeadAttentionAlibi):\n",
    "            handle = layer.register_forward_hook(hook)\n",
    "            handles.append(handle)\n",
    "    \n",
    "    return attention_weights, handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights\n",
    "attention_weights, handles = get_attention_weights(model.model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 2, 16000])\n",
      "Error during model forward pass: not enough values to unpack (expected 3, got 0)\n",
      "Number of attention weights captured: 0\n",
      "No attention weights were captured. The model might not store attention weights directly.\n",
      "\n",
      "Model structure:\n",
      "encoder: <class 'vap.modules.encoder.EncoderCPC'>\n",
      "  encoder: <class 'vap.modules.encoder_components.CPCModel'>\n",
      "    gEncoder: <class 'vap.modules.encoder_components.CPCEncoder'>\n",
      "      conv0: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm0: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv1: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm1: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv2: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm2: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv3: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm3: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "      conv4: <class 'torch.nn.modules.conv.Conv1d'>\n",
      "      batchNorm4: <class 'vap.modules.encoder_components.ChannelNorm'>\n",
      "    gAR: <class 'vap.modules.encoder_components.CPCAR'>\n",
      "      baseNet: <class 'torch.nn.modules.rnn.LSTM'>\n",
      "  downsample: <class 'torch.nn.modules.container.Sequential'>\n",
      "    0: <class 'einops.layers.torch.Rearrange'>\n",
      "    1: <class 'vap.modules.encoder_components.CConv1d'>\n",
      "      pad: <class 'torch.nn.modules.padding.ConstantPad1d'>\n",
      "    2: <class 'vap.modules.encoder_components.LayerNorm'>\n",
      "      ln: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "      in_rearrange: <class 'einops.layers.torch.Rearrange'>\n",
      "      out_rearrange: <class 'einops.layers.torch.Rearrange'>\n",
      "    3: <class 'torch.nn.modules.activation.GELU'>\n",
      "    4: <class 'einops.layers.torch.Rearrange'>\n",
      "transformer: <class 'vap.modules.modules.TransformerStereo'>\n",
      "  ar_channel: <class 'vap.modules.modules.GPT'>\n",
      "    layers: <class 'torch.nn.modules.container.ModuleList'>\n",
      "      0: <class 'vap.modules.modules.TransformerLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "  ar: <class 'vap.modules.modules.GPTStereo'>\n",
      "    layers: <class 'torch.nn.modules.container.ModuleList'>\n",
      "      0: <class 'vap.modules.modules.TransformerStereoLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ln_src_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha_cross: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha_cross\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "      1: <class 'vap.modules.modules.TransformerStereoLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ln_src_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha_cross: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha_cross\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "      2: <class 'vap.modules.modules.TransformerStereoLayer'>\n",
      "        dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "        ln_self_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        ln_ffnetwork: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ffnetwork: <class 'torch.nn.modules.container.Sequential'>\n",
      "          0: <class 'torch.nn.modules.linear.Linear'>\n",
      "          1: <class 'torch.nn.modules.activation.GELU'>\n",
      "          2: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          3: <class 'torch.nn.modules.linear.Linear'>\n",
      "        ln_src_attn: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "        mha_cross: <class 'vap.modules.modules.MultiHeadAttentionAlibi'>\n",
      "          Attention layer found: mha_cross\n",
      "          key: <class 'torch.nn.modules.linear.Linear'>\n",
      "          query: <class 'torch.nn.modules.linear.Linear'>\n",
      "          value: <class 'torch.nn.modules.linear.Linear'>\n",
      "          unstack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          stack_heads: <class 'einops.layers.torch.Rearrange'>\n",
      "          attn_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          resid_drop: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "          proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "    combinator: <class 'vap.modules.modules.Combinator'>\n",
      "      h0_a: <class 'torch.nn.modules.linear.Linear'>\n",
      "      h0_b: <class 'torch.nn.modules.linear.Linear'>\n",
      "      ln: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "      activation: <class 'torch.nn.modules.activation.GELU'>\n",
      "objective: <class 'vap.objective.VAPObjective'>\n",
      "  codebook: <class 'vap.objective.Codebook'>\n",
      "    emb: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "feature_projection: <class 'torch.nn.modules.linear.Identity'>\n",
      "va_classifier: <class 'torch.nn.modules.linear.Linear'>\n",
      "vap_head: <class 'torch.nn.modules.linear.Linear'>\n",
      "\n",
      "Model output:\n",
      "x: torch.Size([1, 50, 256])\n",
      "x1: torch.Size([1, 50, 256])\n",
      "x2: torch.Size([1, 50, 256])\n",
      "logits: torch.Size([1, 50, 256])\n",
      "vad: torch.Size([1, 50, 2])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the correct input shape\n",
    "dummy_input = torch.randn(1, 2, 16000)  # Batch size 1, 2 channels, 16000 samples (1 second at 16kHz)\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "\n",
    "# Process the dummy input through the model\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        outputs = model(dummy_input)\n",
    "        print(\"Forward pass successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model forward pass: {str(e)}\")\n",
    "\n",
    "# Remove the hooks\n",
    "for handle in handles:\n",
    "    handle.remove()\n",
    "\n",
    "print(f\"Number of attention weights captured: {len(attention_weights)}\")\n",
    "\n",
    "if attention_weights:\n",
    "    print(\"Attention weights captured successfully\")\n",
    "    # Here you can add visualization code if needed\n",
    "else:\n",
    "    print(\"No attention weights were captured. The model might not store attention weights directly.\")\n",
    "\n",
    "# Print model's structure\n",
    "print(\"\\nModel structure:\")\n",
    "def print_model_structure(model, depth=0):\n",
    "    for name, module in model.named_children():\n",
    "        print(\"  \" * depth + f\"{name}: {type(module)}\")\n",
    "        if isinstance(module, MultiHeadAttentionAlibi):\n",
    "            print(\"  \" * (depth+1) + f\"Attention layer found: {name}\")\n",
    "        print_model_structure(module, depth + 1)\n",
    "\n",
    "print_model_structure(model.model)\n",
    "\n",
    "# If no attention weights were captured, let's print the output of the model\n",
    "if not attention_weights:\n",
    "    print(\"\\nModel output:\")\n",
    "    for key, value in outputs.items():\n",
    "        print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No attention weights were captured. The model might not use standard attention layers.\n"
     ]
    }
   ],
   "source": [
    "if attention_weights:\n",
    "    # Visualize attention weights\n",
    "    num_layers = len(attention_weights)\n",
    "    fig, axes = plt.subplots(num_layers, 1, figsize=(10, 5 * num_layers))\n",
    "    if num_layers == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for layer, weights in enumerate(attention_weights):\n",
    "        avg_weights = weights.mean(0).cpu().numpy()\n",
    "        im = axes[layer].imshow(avg_weights, aspect='auto', cmap='viridis')\n",
    "        axes[layer].set_title(f'Average Attention Weights - Layer {layer+1}')\n",
    "        axes[layer].set_xlabel('Key Position')\n",
    "        axes[layer].set_ylabel('Query Position')\n",
    "        fig.colorbar(im, ax=axes[layer])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attention_weights.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Visualization complete. Check 'attention_weights.png' for the result.\")\n",
    "else:\n",
    "    print(\"No attention weights were captured. The model might not use standard attention layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
